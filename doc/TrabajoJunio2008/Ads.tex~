\documentclass{article}

%\usepackage{nips06submit,times}
%\documentstyle[nips06submit_09,times]{article}


%\documentclass[a4paper,10pt]{article}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{psfrag}
\usepackage{epsf}
\usepackage{amssymb}
\usepackage{verbatim}
\usepackage{epsfig}
\usepackage{subfigure}
\usepackage[lined,algonl,boxed]{algorithm2e}

\newtheorem{definition}{Definition}

\begin{document}
\title{Toward efficient wrapper algorithms for high-dimensional feature selection}
\author{
Antonio Jes\'us Adsuar\\
adsuar@lsi.upc.edu\\
Departament de Llenguatges i Sistemes Inform\`atics\\
Universitat Polit\`ecnica de Catalunya\\
Barcelona, Spain}

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vfil\noindent
{\huge Toward efficient wrapper algorithms for high-dimensional feature selection}

\vfil\noindent
{\Large\em Antonio Jes\'us Adsuar}\\
adsuar@lsi.upc.edu\\
\vfil

\noindent
Departament de Llenguatges i Sistemes Inform\`atics\\
Universitat Polit\`ecnica de Catalunya\\
Barcelona, Spain
\vfil
{\flushright Trabajo presentado para la obtenci\'on de 3 cr\'editos de
investigaci\'on tutelada}\\
Direcci\'on: Llu\'{\i}s A. Belanche Mu\~{n}oz
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\date{June 22, 2008}  
\maketitle

\begin{abstract}
\noindent
This research addresses the problem of selecting a subset of features from
a given set in an inductive learning setting. An important family of
algorithms perform an explicit search in the space of subsets by
iteratively adding and/or removing features one at a time until some
condition is met. This process is guided by an evaluation measure
(usually the inducer itself).

is tested in combination
with several machine learning algorithms on five public domain
microarray data sets. It is found that this combination offers subsets
yielding similar or much better accuracies than using the full set of
genes. The obtained solutions are of comparable quality to previous results, 
but they are obtained in a maximum of half an hour computing
time and use a very low number of genes.

A well-balanced algorithm is one that maintains a good trade-off
between compact, competitive and efficiently found solutions. 


\end{abstract}

\newpage
\tableofcontents
\newpage

\section{Introduction}
\label{section:Introduction}

The importance of feature selection is beyond doubt in inductive
machine learning and data mining \cite{LM98}. The feature
selection problem in terms of supervised inductive learning is: given
a set of candidate features select a subset defined by one of three
approaches: a) the subset with a specified size that optimizes an
evaluation measure, b) the subset of smaller size that satisfies a
certain restriction on the evaluation measure and c) the subset with
the best commitment among its size and the value of its evaluation
measure (general case). The generic purpose pursued is the improvement
of the inductive learner, either in terms of learning speed,
generalization capacity or simplicity of the representation. Other
advantages are easier understanding of the results obtained by the
inducer and reduction of the noise generated by irrelevant or
redundant features, thereby eliminating useless knowledge.


A feature selection algorithm can be seen as a computational solution
motivated by a certain definition of \emph{relevance}.  However, the
relevance of a feature --as seen from the inductive learning
perspective-- may have several definitions depending on the objective
that is looked after. An irrelevant feature is not useful for
induction, but not all relevant features are necessarily useful for
induction \cite{CF94b}. As clear as it is, this fact is also
dependent on the definition of relevance itself. Many times the
inducer itself is used as the measure of relevance, a tactic that has
recently been been coined as {\em wrapper method} \cite{JKP94}.

In the literature, several suboptimal algorithms have been proposed
for feature selection. A wide family is formed by those algorithms
which depart from an initial solution and iteratively add or delete
features by locally optimizing an evaluation measure. Here the word
{\em local} has to be understood as ``in a neighbourhood of the
current solution''. These algorithms leave a sequence of visited
states (there is no backtracking) and simply iterate until some stopping condition
is met (e.g., end of resources). They can be cast in the general class
of {\em hill-climbing algorithms} if one accepts that each iteration of the main loop can
actually worsen the evaluation measure (i.e., they are non-monotonic).

In this research, we review ...

These two new algorithms seek a
better compromise between final subset size, quality of the solution
(as given by the evaluation measure) and computational efficiency.

... are studied to assess their performance ...

The results ...

The document is organized as follows: in section
\ref{section:FeatureSelection} we briefly review the feature selection
problem. In section ... Next,
the two little oscillating algorithms are introduced in section
\ref{section:Littles}. The methodology and tools used for a through
empirical evaluation are covered in section
\ref{section:ExperimentalDesign}, and finally the results are
presented and discussed in section \ref{section:results}. The paper ends with
the conclusions and directions for future work.


\section{The Feature Selection problem}
\label{section:FeatureSelection}

Let $X = \{x_1, \ldots, x_n\}, n>0$ denote the full or universal
feature set.  Without loss of generality, we assume that the
evaluation measure $J:{\cal P}(X) \rightarrow \mathbb{R}^{+} \cup \{ 0
\} $ is to be maximized, where ${\cal P}$ denotes the set of parts. At
all times in the discussion, $X_k \subseteq X$ denotes a subset of
\textit{currently selected} features, with $|X_k| = k$. Hence, by
definition, $X_0 = \emptyset$, and $X_n = X$.

\begin{definition}[Feature Selection] 
\label{def-fs}
Let $J$ be an evaluation measure to be optimized (say to
maximize). The selection of a feature subset can be made under two
premises:

\begin{itemize}
 \item Set $0 < m < n$. Find  $X_m \subset X$, such that
$J(X_m)$ is maximum.
\item Set a real value $J_{min}$, this is, the minimum $J$ that is going to be
  accepted. Find the $X_m \subseteq X$ with smaller $m$ such that
  $J(X_m) \geq J_{min}$. Alternatively, given $\epsilon>0$, find the $X_m
  \subseteq X$ with smaller $m$, such that $|J(X_m) - J(X)| <
  \epsilon J(X)$.
\end{itemize}
\end{definition}

Notice that, with this definition, the optimal subset of features
always exists but is not necessarily unique. Also noteworthy is the
fact that, denoting by $X^*$ one of the optimal solutions, either of
$J(X^*) > J(X)$, $J(X^*) = J(X)$, $J(X^*) < J(X)$ may occur.

As stated, the purpose of a feature selection algorithm is to identify
relevant features according to a definition of relevance. In the
following, we assume the existence of a way of computing the relevance
of a set of features. This relevance function (which may actually be
an heuristic approximation of an otherwise unknown value) is captured by the
function $J$.

\section{Microarray}
Traditional methods of tackling the problem of distinctions between
different types of cancer are primarily based on morphological
characteristics of tumorous tissue \cite{Chuf05}. \cite{Go99}
pioneered work in cancer classification by gene expression using DNA
microarray and showed the possibility to help the diagnosis by means
of mathematical or statistical techniques. Machine Learning or more
generally Data Mining methods are now extensively used for this task
\cite{Bo05}. However, in this setting gene expression data analysis
entails a heavy computational consumption of resources, due to the
extreme sparseness compared to standard data sets in classification
tasks \cite{Tan07}. Typically, a gene expression data set may consist
of dozens of observations but with thousands or even tens of thousands
of genes. Classifying cancer types using this very high ratio between
number of variables and number of observations is a delicate process.
As a result, dimensionality reduction and in particular \emph{feature
  selection} techniques may be very useful. The main purpose of
feature selection is to find a reduced set of features from a data set
described by a feature set. This is often carried out with a {\em
  subset search process} in the power set of possible solutions,
guided by the optimization of a user-defined {\em objective function}.
The generic purpose pursued is the improvement in the generalization
capacity of the \emph{inductive learner} by reduction of the noise
generated by irrelevant or redundant features. The Bioinformatics
community has recognized the feature selection process as a key issue
in gene expression data analysis \cite{Gian07}.

In this work we contribute with ...
as a fast method to generate a potentially useful subset of genes aimed at finding useful feature subsets
evaluated as a whole with respect to classification ability of tumors,
rather than ranking individual contribution (that implicitly denies
interaction between features). 

The ... is tested in this work in combination with four machine
learning algorithms on five public domain microarray data sets. It is
found that this combination offers subsets yielding similar or much
better accuracies than using the full set of genes. Comparing previous
results to ours, the solutions are of similar or sometimes lower
quality but they are obtained in a maximum of half an hour computing
time and the offered accuracy is attained with a very low number of
genes.

In a Microarray Gene Expression context, there is a wide spectrum of
feature selection algorithms that are categorized into one of the three
main forms mentioned above. Traditional gene selection methods select
the \emph{top ranked} features based on some performance measure of
individual classification \cite{Rui05}. This is in general a fast
approach, but considering individual contributions only can very
likely hinder the discovery of possible interactions between genes.

Among the inducers used in wrapper mode, classifiers like the
\emph{Support Vector Machine} \cite{Vap98} are increasingly used as a
reliable tool to evaluate gene selection process --see e.g. 
\cite{Bo05, Tan07, Sat06, Chuf05}. Arguably, the two main 
drawbacks are the necessary parameter tuning process (kernel and
regularization parameters) and the computational burden.

Among the parameter-free classifiers, the \emph{Nearest Neighbor}
\cite{Th05} and \emph{Na\"ive Bayes} techniques \cite{To04} are
examples of classifiers applied with some success in cancer microarray
data.  The former has the peculiarity of being sensitive to the local
structure of the data. In spite of assuming strong (na\"ive)
independence assumptions about the features, the latter has shown also
good results in classification performance. Information-theory based
algorithms constitute also an important choice in feature selection
algorithms in \emph{filter mode}, as described below. Recent work on
Microarray Gene Expression can be found in \cite{Rui05},
\cite{Man05} and \cite{Lei04} among others.

\section{The Granularity and Sweep Search algorithms}
\label{section:Littles}
In this section we introduce two new algorithms that we have designed.
They are meant to be faster than well-known algorithms but, without losing their nearness to the optimal solution. The intention is to retain the good search abilities with a much lower computational cost and simplicity.

The sense of these two new approaches is to work in reduced portions of the feature space, so the calculations will be faster and, has still to be proved, its goodness at J evaluations (that's the main purpose of this paper) will be improved.

The representation of the feature space, will be by means of the use of X, a vector in the way: $X=\{X_1,...,X_n\}$, where $X_i \in \{0,1\}$. Given $J:P(X) \rightarrow[0,1]$, the purpose is to maximize $J$.

\subsection{Granularity Search}
The first of this two approaches is \textbf{Granularity Search Algorithm} (from now on, will be known as \textbf{GSA}). We'll proceed to show the algorithm (Algorithm \ref{GSA}) to, subsequently, expose the strengths and weakenesses of it.


\begin{algorithm}
\dontprintsemicolon
\KwData{$X$ such that $|X|=features$}
\KwResult{$\tilde{k},\tilde{J}$}
\Begin{
   $initialize(X,0.0)$\;
   $estimation \longleftarrow -1$\;
   $\tilde{estimation} \longleftarrow -1$\;
   $\tilde{estimation}_{old} \longleftarrow -1$\;
   $\tilde{k} \longleftarrow -1$\;
   $\tilde{k}_{old} \longleftarrow -1$\;
   $Z \longleftarrow \emptyset$\;
   \BlankLine
   \For{$i \leftarrow features$ \KwTo $1$}
   {
      $X_{aux} \longleftarrow X$\;
      \BlankLine
      \If{$numberOfFeatures(X_{aux}) > 0$} {
         $\tilde{estimation} \longleftarrow J(X_{aux})$\;
         $\tilde{k} \longleftarrow numberOfFeatures(X_{aux})$\;
      }
      \BlankLine
      \Repeat{$\tilde{estimation} \ne \tilde{estimation}_{old}$}{
         $Z \longleftarrow vector(i)$\;
         $initialize(X,1.0)$\;
         \BlankLine
            $\tilde{estimation}_{old} \longleftarrow \tilde{estimation}$\;
            $\tilde{k}_{old} \longleftarrow \tilde{k}$\;
            \BlankLine
            \For{$j  \leftarrow i$}{
               $invert(j,Z,X_{aux})$\;
               $estimation \longleftarrow J(X_{aux})$\;
               \BlankLine
               \If{$estimation>\tilde{estimation}$ {\bf or}$(estimation=\tilde{estimation}$ {\bf and} $numberOfFeatures(X_{aux})<\tilde{k})$}{
                  $\tilde{estimation} \longleftarrow estimation$\;
                  $\tilde{k} \longleftarrow numberOfFeatures(X_{aux})$\;
                  $Z[j] \longleftarrow 0$\;
               }
               \lElse{
                  $invert(j,Z,X_{aux})$\;
               }
            }
         }
      \BlankLine
      $X \longleftarrow X_{aux}$\;
   }
}
\caption{Granularity Search Algorithm\label{GSA}}
\end{algorithm}

The purpouse of this algorithm is to work with the space of features splitting this into different subsets, the size of whom will grow exploring not only the strength of each feature, but also the relation between the rest of the feature space.

The way of studying the relation between a subset of features and the rest of the feature space is through the use of Z. Z it's related with X, and it's a binary vector of size $k$, where $k$ is the number of subdivisions of $X$. When $Z_i=1$, we take as it is the whole subset. Otherwise, we take the negation of each value of the subset.

$GS$ starts with $k=|X|$ (so there'll be as many subsets as features), and will study the convenience of being $Z_i$ 0 or 1, $\forall i \in \{1,|Z|\}$.

\subsection{Sweep Search}
The later of the approaches is \textbf{Sweep Search Algorithm} (from now on, \textbf{SSA}). In the same way we discussed about \textbf{GSA}, we'll proceed to show the algorithm to, subsequently, expose the strengths and weakenesses of it.

\begin{algorithm}
\dontprintsemicolon
\KwData{$X$ such that $|X|=features,sizeOfSubsets,treatmentOfSubsets,$\\

$algorithmForSubsets$}
\KwResult{$\tilde{k},\tilde{J}$}
\Begin{
   \Switch{$treatmentOfSubsets$}{
      \lCase{all values to 0}{ $initialize(X,0.0)$\;
         $break$\;
      }
      \lCase{all values to 1}{ $initialize(X,1.0)$\;
         $break$\;
      }
      \lCase{random values}{ $randomInitialize(X,0,1)$\;
         $break$\;
      }
      \lCase{last done}{ $initialize(X,0.0)$\;
         $break$\;
      }
   }
   \BlankLine
   $X \longleftarrow X_{aux}$\;
   $numberOfDivisions \longleftarrow features/sizeOfSubsets$\;
   \BlankLine
   \For{$i\leftarrow0$ \KwTo $numberOfDivisions$}{
      $applyAlgorithmForSubset(algorithmSubsets,i,$\\ \ \ $sizeOfSubsets,X_{aux})$\;
      \BlankLine
      \Switch{$treatmentOfSubsets$}{
         \Case{all values to 0}{
            $copySubset(j,X_{aux},X)$\;
            $restoreSubset(j,X_{aux},0.0)$\;
            $break$\;
         }
         \Case{all values to 1}{
            $copySubset(j,X_{aux},X)$\;
            $restoreSubset(j,X_{aux},0.0)$\;
            $break$\;
         }
         \lCase{random values}{\;}
         \Case{last done}{
            $copySubset(j,X_{aux},X)$\;
            $break$\;
         }
      }

   }
}
\caption{Sweep Search Algorithm\label{SSA}}
\end{algorithm}


\section{An Empirical Evaluation of GPTAs}
\label{section:ExperimentalDesign}

The first question arising in an experimental design for feature
selection performance evaluation is: what are the aspects that we
would like to evaluate of a GPATA solution in a given data set?  A
well-balanced or {\em good} algorithm is one that maintains a good trade-off between
compact, competitive and efficiently found solutions. 
The aim of the experiments is precisely to contrast the ability of the different GPATAs presented in terms of 
accuracy, size and efficiency.

We introduce a {\em goodness criterion}:

Let $\tilde{k}$ be the size of the final solution yielded by a GPTA and
$\tilde{X}_{\tilde{k}} \subseteq X$ the solution itself. Make
$\tilde{J}_k = J(\tilde{X}_{\tilde{k}})$. Call $\#J$ the number of times
the function J has been needed in the full execution. The problem is:

\begin{equation}
\label{tradeoff}
G(X')= \left( \frac{1}{\tilde{J}_k+1} +
\frac{\tilde{k}}{n}\right) log_2 (\#J+1)
\end{equation}

where $log_2$ is the base 2 logarithm. The problem is to solve for 
$$  X^* = arg \min\limits_{X' \subseteq X} G(X')$$

Note $G(X') \in (0,\infty)$.

\subsection{Experimental design}
In this subsection we detail the experimental methodology and quantify
the various parameters of the experiments. 


\subsection{Problems tested}
\label{section:datos}

Five real microarray data sets are used in this work which are shortly described below.

\begin{enumerate}
\item \emph{Colon Tumor}: Used originally by
  \cite{Al99}, it consists of 62 observations of colon tissue, of
  which 40 are tumorous and 22 normal, and contains 2,000 genes. This
  particular data set does not come with pre-established training and
  test samples. Therefore, it was randomly split in $2/3$ (41
  observations) and $1/3$ (21 observations) respectively, as in recent
  works \cite{Bu07, Tod04}, keeping the same proportion of
  classes in both sets.
\item \emph{Leukemia}: Published by \cite{Go99}, the
  training set has 38 bone marrow observations and 7,129 probes:
  6,817 human genes and 312 control genes. A test set is supplied with
  34 observations. The goal is to tell acute myeloid leukemia
  (AML) from acute lymphoblastic leukemia (ALL).
\item \emph{Lung Cancer}: Obtained by \cite{Gor02}, the
  problem consists in distinguishing between malignant pleural mesothelioma
  and adenocarcinoma of lung; 181 observations with 12,533 genes are
  distributed in the training set (32 observations) and the test set (181).
\item \emph{Prostate Cancer}: This data set was used by
  \cite{Sig02} to analyze differences in pathological features of
  prostate cancer and to identify genes that might anticipate its
  clinical behavior; 136 observations and 12,600 genes are included,
  with 102 training and 34 test observations.
\item \emph{Breast Cancer}: \cite{Ver02} studied 97
  patients with primary invasive breast carcinoma; 12,600 genes were
  analyzed in 78 training observations and tested on 18 observations.
\end{enumerate}




\section{Results}
%\subsection{Discussion of the results}
The gene selection process by the EFA is shown in Table \ref{Tab:BGS}.
Remarkably, it produces BGSs with very low size and getting
the maximum value of $R=1$ for each of the problems.  The lowest sizes
are attained for the \emph{Leukemia} and \emph{Lung Cancer} (only 2
genes).  The processing time is acceptably low in all cases, spanning
from half a minute for \emph{Colon Tumor} to half an hour for
\emph{Breast Cancer}, obtained in an average single-CPU laptop. In
spite of the relativity of absolute processing times to the type of
computer, these processing times are very acceptable considering the
high quantity of genes evaluated.


\begin{table*}[!ht]
  \centering
  \begin{footnotesize}
  \begin{tabular}{|@{$~$}l@{$~$}|@{$~$}c@{$~$}|@{$~$}c@{$~$}|@{$~$}r@{$~$}|@{$~$}l@{$~$}|}
   \hline
   {\it Data set}   &$|BGS|$&max. $R$&Time (secs.)&Gene Accession
   Number (GAN) or Gene name\\
   \hline
   Colon Tumor     & 3& 1&  33.25& R87126, H27277, T47377\\
   Leukemia        & 2& 1&  51.50& U79725, X95735\\
   Lung Cancer     & 2& 1&  81.88& 2047\_s\_at, 38139\_at\\
   Prostate Cancer & 5& 1& 831.33& AF059274, X07732, AF006516, N73769, D83018\\
   Breast Cancer   & 4& 1&1842.23& AA682425, AL137692, AI082692, NM\_001216\\
   \hline
  \end{tabular}
  \end{footnotesize}
  \caption{Genes selected by the EFA. \textbf{$|BGS|$} is the number
    of genes.}\label{Tab:BGS}
\end{table*}

The results of the classification experiments are presented in Table
\ref{Tab:Acc}. The reported measure of performance is accuracy of the
different classifiers (built using only the genes in the BGSs) on the
respective test sets. It is seen that performance, as compared to
that obtained without reduction (column NR) is increased in most
cases, sometimes very substantially.

\begin{table*}[!htb]
  \centering
  \begin{footnotesize}
  \begin{tabular}{|@{~}c@{~}|c@{~}c|c@{~}c|c@{~}c|c@{~}c|c@{~}c@{~}|c@{~}c@{~}|c@{~}c@{~}|}
  \hline
          & \multicolumn{ 2}{|c}{1NN} & \multicolumn{ 2}{|c}{NB} & \multicolumn{ 2}{|c}{C4.5} & \multicolumn{ 2}{|c}{RF} & \multicolumn{ 2}{|c}{\cite{Bu07}} & \multicolumn{ 2}{|c}{\cite{Bo05}} & \multicolumn{ 2}{|c|}{\cite{To04}} \\
  \cline{2-15}
  Data set &   NR &  BGS &   NR &  BGS &   NR &  BGS &   NR &  BGS &  Acc. &Size &  Acc. &Size &  Acc. &Size\\
  \hline
  Colon Tumor     & 0.24 & 0.81 & 0.19 & 0.81 & 0.90 & 0.81 & 0.48 & 0.81 &  0.77 &  33 &  0.93 &   3 &  0.78 &   6 \\
  Leukemia        & 0.97 & 0.94 & 0.74 & 0.94 & 0.94 & 0.94 & 0.97 & 0.94 &  0.96 &  30 &  0.97 &  37 &  0.90 &   3 \\
  Lung Cancer     & 0.92 & 0.93 & 0.99 & 0.93 & 0.93 & 0.93 & 0.98 & 0.93 &  0.99 &  38 &  0.99 &  33 &     - &   - \\
  Prostate Cancer & 0.85 & 0.74 & 0.26 & 0.79 & 0.53 & 0.71 & 0.91 & 0.71 &  0.93 &  47 &     - &   - &     - &   - \\
  Breast Cancer   & 0.74 & 0.79 & 0.63 & 0.79 & 0.68 & 0.68 & 0.68 & 0.79 &  0.79 &  46 &  0.96 & 161 &     - &   - \\
  \hline
\end{tabular}
  \end{footnotesize}
  \caption{Test set average accuracy returned by the nearest-neighbor
    technique with Euclidean metric (\emph{1NN}), the \emph{Na\"{\i}ve
      Bayes} (NB), a \emph{C4.5} decision tree and a Random Forest
    (RF), plus comparison references. The column \textbf{NR} indicates
    ccuracy results with no reduction of features, whereas
    \textbf{BGS} are the accuracy results with the BGSs.}
\label{Tab:Acc}
\end{table*}


We also present a comparison to other approaches in Table
\ref{Tab:Acc}. Specifically, three references, representative of
recent and relatively successful work, are used as a comparison with
the results obtained by the EFA. Care has to be exercised in looking
at these figures, since experimental conditions and goals are not
necessarily the same. The computing times are not reported in any of
these references, making a fair comparison more difficult. However,
the kind of approach used therein (a wrapper algorithm involving a SVM
or an evolutionary algorithm, among other techniques) makes us think
that they entail a much heavier computational burden. It is plain to
see that the other references get generally higher accuracy values
than those reached by EFA, though none of them is a clear winner,
since medium-to-small differences in performance should not be taken
as significant given the tiny number of test set observations. A
further interesting point is that these higher accuracy values are the
result of higher BGS sizes, a fact that has two important
consequences. On the one hand, the risk of an overfit solution is
high. On the other hand, it makes interpretation much more difficult.
This issue is analyzed in the following section.

\section{Discussion}
\label{section:results}

The results shown are those in Figs. \ref{fig:experimentos1} and
\ref{fig:experimentos2}.  These plots allow a human-eye quick
comparison across all the sample datasets with respect to each studied
particularity (final subset size $k$, final subset evaluation $J$ and
final subset score $s$) on a given pair of algorithms. We recall that
a good solution is characterized by a low $k$ and a high $J$ and $s$.


\section{Conclusions and future work}
In this work we have introduced two new ...
which can be seen as much simplified versions of oscillating
algorithms (hence called {\em little oscillating algorithms}).

A well-balanced algorithm is one that maintains a good trade-off
between compact, competitive and efficiently found solutions. We carry
out an extensive experimental study on synthetic problems in order to
assess the behaviour of the algorithms in these terms (accuracy and size)
keeping efficiency as similar as possible. Future studies could keep
other values constant (e.g. $k$) or generally the three terms free.

The results illustrate the differential behaviour of the algorithms
tested, specially in terms of the accuracy and size obtained for
similar efficiency. The two little oscillating algorithms do not
escape from this fact. Therefore, the obtention of a balanced
algorithm in this respect is still a challenging research avenue. We
are specially interested in the design of an ``all terrain'' algorithm
having a good overall balance that may serve as a first choice. Then,
a more specialized algorithm can be used in view on the results
obtained.



An important aim is to ascertain whether there really is a difference
in the long run in using one algorithm over another and, if this is
the case, to what degree and in what respects. To sum up, we
contemplate a mid-term research goal in the design of new, better
algorithms, where by better we understand a good compromise between
final subset size, quality of the solution (as given by the evaluation
measure) and computational efficiency.

\begin{thebibliography}{TBB}


\bibitem[CF94b]{CF94b}
R.~A. Caruana and D.~Freitag.
\newblock {How Useful is Relevance?}
\newblock Technical report, {Fall'94 AAAI Symposium on Relevance}, {New
  Orleans}, 1994.


\bibitem[DK82]{DK82}
P.~A. Devijver and J.~Kittler.
\newblock {\em {Pattern Recognition -- A Statistical Approach}}.
\newblock {Prentice Hall}, {London, GB}, 1982.


\bibitem[JKP94]{JKP94}
G.~H. John, R.~Kohavi, and K.~Pfleger.
\newblock {Irrelevant Features and the Subset Selection Problem}.
\newblock In {\em {Proc. of the 11th International Conference on Machine
  Learning}}, pages 121--129, {New Brunswick, NJ}, 1994. {Morgan Kaufmann}.

\bibitem[LM98]{LM98}
H.~Liu and H.~Motoda.
\newblock {\em {Feature Selection for Knowledge Discovery and Data Mining}}.
\newblock {Kluwer Academic Publishers}, {London, GB}, 1998.

\bibitem[MBN02]{MBN02}
Molina, L.C. Belanche, L.; Nebot, �.  Contribuciones a la Evaluaci�n
de Algoritmos de Selecci�n de Atributos para Problemas de Aprendizaje
Inductivo.  Research Report LSI-02-05-T, LSI-UPC, Barcelona, Espa�a,

\bibitem[PATAwebsite]{PATAwebsite}
{\tt http://www.lsi.upc.edu/\%7eadsuar/pata/}

\bibitem[PNK94]{PNK94}
P.~Pudil, J. Novovicov\'a, and J.~Kittler.
\newblock {Floating Search Methods in Feature Selection}.
\newblock {\em {Pattern Recognition Letters}}, 15(11):1119--1125, 1994.

\bibitem[SPNP99]{SPNP99}
P.~Somol, P.~Pudil, J. Novovicov\'a, and P.~Pacl\'ik.
\newblock {Adaptative Floa\-ting Search Methods in Feature Selection}.
\newblock {\em {Pattern Recognition Letters}}, 20(11--13):1157--1163, 1999.


\bibitem[Tetal91]{Tetal91}
S.~B. Thrun, J.~Bala, E.~Bloedorn, I.~Bratko, B.~Cestnik, J.~Cheng, K.~De~Jong,
  S.~D\v~zeroski, S.~E. Fahlman, D.~Fisher, R.~Hamann, K.~Kaufman, S.~Keller,
  I.~Kononenko, J.~Kreuziger, R.~S. Michalski, T.~Mitchell, P.~Pachowicz,
  Y.~Reich, H.~Vafaie, W.~Van~de Welde, W.~Wenzel, J.~Wnek, and J.~Zhang.
\newblock {The {MONK}'s Problems: {A} Performance Comparison of Different
  Learning Algorithms}.
\newblock Technical Report CS-91-197, Carnegie Mellon University, Pittsburgh,
  PA, 1991.

\end{thebibliography}


\end{document}